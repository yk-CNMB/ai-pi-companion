# =======================================================================
# Pico AI Server - ULTIMATE FULL VERSION
# åŒ…å«ï¼šå…¨åŠŸèƒ½ TTSã€åŒæ¨¡è°ƒç”¨ã€å®Œæ•´é”™è¯¯å¤„ç†ã€è¯¦ç»†æ—¥å¿—ã€äººè®¾ä¿æŠ¤
# =======================================================================
import os
import json
import uuid
import time
import glob
import shutil
import re
import zipfile
import threading
import requests
import urllib.parse
import base64
import asyncio
import logging
import subprocess
import sys
import traceback  # å¢åŠ é”™è¯¯å †æ ˆæ‰“å°

# æ ¸å¿ƒä¾èµ–åº“æ£€æŸ¥
try:
    import edge_tts
    print("âœ… edge_tts åº“åŠ è½½æˆåŠŸ")
except ImportError:
    print("âŒ è­¦å‘Š: æœªæ‰¾åˆ° edge_tts åº“ï¼Œå°†å°è¯•ä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å¼")

from flask import Flask, render_template, request, make_response, redirect, url_for, jsonify
from flask_socketio import SocketIO, emit, join_room, leave_room
from google import genai
from google.genai import types
from werkzeug.utils import secure_filename

# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='[%(asctime)s] %(levelname)s in %(module)s: %(message)s'
)

app = Flask(__name__, static_folder='static')
app.config['SECRET_KEY'] = 'pico_ultimate_secret_key_2025'
app.config['MAX_CONTENT_LENGTH'] = 200 * 1024 * 1024  # 200MB ä¸Šä¼ é™åˆ¶

# SocketIO é…ç½® - å¢åŠ  buffer é˜²æ­¢å¤§å›¾æ–­è¿
socketio = SocketIO(app, 
    cors_allowed_origins="*", 
    async_mode='threading', 
    ping_timeout=60, 
    ping_interval=25,
    max_http_buffer_size=100*1024*1024
)

SERVER_VERSION = str(int(time.time()))

# --- ç›®å½•ç»“æ„åˆå§‹åŒ– ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
AUDIO_DIR = os.path.join(BASE_DIR, "static", "audio")
MODELS_DIR = os.path.join(BASE_DIR, "static", "live2d")
BG_DIR = os.path.join(BASE_DIR, "static", "backgrounds")
STATE_FILE = os.path.join(BASE_DIR, "server_state.json")
CONFIG_FILE = os.path.join(BASE_DIR, "config.json")

# ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å­˜åœ¨
for d in [AUDIO_DIR, MODELS_DIR, BG_DIR]:
    if not os.path.exists(d):
        try:
            os.makedirs(d)
            logging.info(f"åˆ›å»ºç›®å½•: {d}")
        except Exception as e:
            logging.error(f"åˆ›å»ºç›®å½•å¤±è´¥ {d}: {e}")

# --- å…¨å±€é…ç½®åŠ è½½ ---
CONFIG = {
    "GEMINI_API_KEY": "",
    "TTS_VOICE": "zh-CN-XiaoxiaoNeural",
    "TTS_RATE": "+0%",
    "TTS_PITCH": "+0Hz"
}

try:
    if os.path.exists(CONFIG_FILE):
        with open(CONFIG_FILE, "r", encoding='utf-8') as f: 
            # è¿‡æ»¤æ‰æ³¨é‡Šè¡Œ (ä»¥ // å¼€å¤´çš„è¡Œ)
            lines = [line for line in f.readlines() if not line.strip().startswith("//")]
            if lines:
                CONFIG.update(json.loads("\n".join(lines)))
                logging.info("é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
except Exception as e:
    logging.error(f"åŠ è½½é…ç½®æ–‡ä»¶å‡ºé”™: {e}")

# åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯
client = None
api_key = CONFIG.get("GEMINI_API_KEY")
if api_key and "AIza" in api_key:
    try:
        client = genai.Client(api_key=api_key)
        logging.info("Gemini å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logging.error(f"Gemini åˆå§‹åŒ–å¤±è´¥: {e}")

# --- é»˜è®¤äººè®¾æŒ‡ä»¤ (ä»…ä½œä¸º fallback) ---
DEFAULT_INSTRUCTION = """
ã€ç³»ç»ŸæŒ‡ä»¤ã€‘
ä½ æ˜¯ä¸€ä¸ªè™šæ‹Ÿä¸»æ’­ã€‚
è¯·åœ¨æ¯æ¬¡å›å¤çš„å¼€å¤´ï¼Œå¿…é¡»æ˜ç¡®æ ‡è®°ä½ å½“å‰çš„å¿ƒæƒ…æ ‡ç­¾ã€‚
æ ‡ç­¾å¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š[HAPPY], [ANGRY], [SAD], [SHOCK], [NORMAL]ã€‚
"""

# --- å…¨å±€çŠ¶æ€ç®¡ç† (æ”¾åœ¨æœ€å‰é˜²æ­¢ NameError) ---
GLOBAL_STATE = { 
    "current_model_id": "default", 
    "current_background": "", 
    "chat_history": [] 
}

def save_state():
    """ä¿å­˜æœåŠ¡å™¨çŠ¶æ€åˆ° JSON æ–‡ä»¶"""
    try:
        with open(STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(GLOBAL_STATE, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logging.error(f"ä¿å­˜çŠ¶æ€å¤±è´¥: {e}")

def load_state():
    """ä» JSON æ–‡ä»¶åŠ è½½æœåŠ¡å™¨çŠ¶æ€"""
    global GLOBAL_STATE
    if os.path.exists(STATE_FILE):
        try:
            with open(STATE_FILE, 'r', encoding='utf-8') as f:
                saved = json.load(f)
                if saved:
                    GLOBAL_STATE.update(saved)
                    # é™åˆ¶å†å²è®°å½•é•¿åº¦ï¼Œé˜²æ­¢æ–‡ä»¶è¿‡å¤§
                    if len(GLOBAL_STATE["chat_history"]) > 100:
                        GLOBAL_STATE["chat_history"] = GLOBAL_STATE["chat_history"][-100:]
            logging.info("æœåŠ¡å™¨çŠ¶æ€åŠ è½½æˆåŠŸ")
        except Exception as e:
            logging.error(f"åŠ è½½çŠ¶æ€å¤±è´¥: {e}")

# ç«‹å³æ‰§è¡ŒåŠ è½½
load_state()

# å½“å‰è¿è¡Œçš„æ¨¡å‹é…ç½®ç¼“å­˜
CURRENT_MODEL = {
    "id": "default", 
    "path": "", 
    "persona": "", 
    "voice": "zh-CN-XiaoxiaoNeural", 
    "rate": "+0%", 
    "pitch": "+0Hz", 
    "scale": 0.5, 
    "x": 0.5, 
    "y": 0.5
}

def get_model_config(mid):
    """
    è¯»å–ç‰¹å®šæ¨¡å‹çš„ config.jsonã€‚
    å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›é»˜è®¤é…ç½®ã€‚
    å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œå®Œå…¨ä¿¡ä»»æ–‡ä»¶å†…å®¹ï¼Œä¸éšæ„è¦†ç›–ã€‚
    """
    p = os.path.join(MODELS_DIR, mid, "config.json")
    
    # é»˜è®¤é…ç½®
    d = {
        "persona": f"ä½ æ˜¯{mid}ã€‚{DEFAULT_INSTRUCTION}", 
        "voice": "zh-CN-XiaoxiaoNeural", 
        "rate": "+0%", 
        "pitch": "+0Hz", 
        "scale": 0.5, 
        "x": 0.5, 
        "y": 0.5
    }
    
    if os.path.exists(p):
        try:
            with open(p, "r", encoding="utf-8") as f: 
                loaded = json.load(f)
                # åªæœ‰å½“åŠ è½½çš„é…ç½®ä¸­æ²¡æœ‰ persona æ—¶ï¼Œæ‰ä½¿ç”¨é»˜è®¤å€¼
                # è¿™æ ·å¯ä»¥ä¿æŠ¤ç”¨æˆ·ä¿®æ”¹è¿‡çš„äººè®¾
                if loaded.get('persona'):
                    d['persona'] = loaded['persona']
                
                # æ›´æ–°å…¶ä»–å­—æ®µ
                d.update(loaded)
        except Exception as e:
            logging.error(f"è¯»å–æ¨¡å‹é…ç½®å¤±è´¥ {mid}: {e}")
            
    return d

def save_model_config(mid, data):
    """ä¿å­˜é…ç½®åˆ°æ¨¡å‹çš„ config.json"""
    p = os.path.join(MODELS_DIR, mid, "config.json")
    
    # å…ˆè¯»å–ç°æœ‰é…ç½®ï¼Œç¡®ä¿ä¸ä¸¢å¤±æœªä¿®æ”¹çš„å­—æ®µ
    curr = get_model_config(mid)
    curr.update(data)
    
    try:
        with open(p, "w", encoding="utf-8") as f:
            json.dump(curr, f, indent=2, ensure_ascii=False)
        logging.info(f"æ¨¡å‹é…ç½®å·²ä¿å­˜: {mid}")
    except Exception as e:
        logging.error(f"ä¿å­˜æ¨¡å‹é…ç½®å¤±è´¥ {mid}: {e}")
        
    return curr

def scan_models():
    """æ‰«ææ‰€æœ‰ Live2D æ¨¡å‹æ–‡ä»¶å¤¹"""
    ms = []
    # éå† live2d ç›®å½•
    for root, dirs, files in os.walk(MODELS_DIR):
        for file in files:
            # å¯»æ‰¾æ¨¡å‹å…¥å£æ–‡ä»¶
            if file.endswith(('.model3.json', '.model.json')):
                full_path = os.path.join(root, file)
                
                # è®¡ç®—ç›¸å¯¹è·¯å¾„
                rel_path = os.path.relpath(full_path, BASE_DIR).replace("\\", "/")
                if not rel_path.startswith("/"): rel_path = "/" + rel_path
                
                folder_name = os.path.basename(os.path.dirname(full_path))
                model_id = folder_name
                
                # é¿å…é‡å¤æ·»åŠ åŒä¸€ä¸ªæ¨¡å‹ ID
                if any(m['id'] == model_id for m in ms): continue
                
                # è·å–è¯¥æ¨¡å‹çš„è¯¦ç»†é…ç½®
                cfg = get_model_config(model_id)
                
                ms.append({
                    "id": model_id, 
                    "name": model_id, 
                    "path": rel_path, 
                    **cfg
                })
    # æŒ‰åç§°æ’åº
    return sorted(ms, key=lambda x: x['name'])

def scan_backgrounds():
    """æ‰«æèƒŒæ™¯å›¾ç‰‡"""
    bgs = []
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.gif']:
        for f in glob.glob(os.path.join(BG_DIR, ext)): 
            bgs.append(os.path.basename(f))
    return sorted(bgs)

def init_model():
    """åˆå§‹åŒ–åŠ è½½æ¨¡å‹"""
    global CURRENT_MODEL
    ms = scan_models()
    
    # å°è¯•æ¢å¤ä¸Šæ¬¡ä½¿ç”¨çš„æ¨¡å‹
    last_id = GLOBAL_STATE.get("current_model_id")
    target = next((m for m in ms if m['id'] == last_id), None)
    
    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•æ‰¾é»˜è®¤çš„
    if not target and len(ms) > 0: 
        target = ms[0]
        
    if target: 
        CURRENT_MODEL = target
        GLOBAL_STATE["current_model_id"] = target['id']
        save_state()
        logging.info(f"å½“å‰æ¨¡å‹åˆå§‹åŒ–ä¸º: {target['id']}")

# åˆå§‹åŒ–æ¨¡å‹
init_model()

# ================= TTS æ ¸å¿ƒ (åŒæ¨¡å†—ä½™è®¾è®¡) =================

def run_edge_tts_cmd(text, output_path, voice, rate, pitch):
    """
    æ–¹å¼1ï¼šå‘½ä»¤è¡Œè°ƒç”¨
    ä¼˜ç‚¹ï¼šè¿›ç¨‹éš”ç¦»ï¼Œä¸å ç”¨ Python GILï¼Œæå…¶ç¨³å®š
    ç¼ºç‚¹ï¼šéœ€è¦ç³»ç»Ÿå®‰è£… edge-tts å‘½ä»¤
    """
    try:
        logging.info(f"[TTS CMD] å¼€å§‹ç”Ÿæˆ: {text[:10]}...")
        cmd = [
            "edge-tts",
            "--text", text,
            "--write-media", output_path,
            "--voice", voice,
            "--rate", rate,
            "--pitch", pitch
        ]
        # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œé˜²æ­¢å¡æ­»
        subprocess.run(cmd, check=True, timeout=15)
        logging.info(f"[TTS CMD] ç”ŸæˆæˆåŠŸ")
        return True
    except subprocess.CalledProcessError as e:
        logging.error(f"[TTS CMD] å‘½ä»¤æ‰§è¡Œé”™è¯¯: {e}")
    except Exception as e:
        logging.error(f"[TTS CMD] æœªçŸ¥é”™è¯¯: {e}")
    return False

def run_edge_tts_python(text, output_path, voice, rate, pitch):
    """
    æ–¹å¼2ï¼šPython åº“è°ƒç”¨
    ä¼˜ç‚¹ï¼šæ— éœ€é…ç½®ç¯å¢ƒå˜é‡ï¼Œç›´æ¥è°ƒç”¨åº“å‡½æ•°
    ç¼ºç‚¹ï¼šåœ¨ Flask çº¿ç¨‹ä¸­éœ€è¦å°å¿ƒå¤„ç† asyncio äº‹ä»¶å¾ªç¯
    """
    try:
        logging.info(f"[TTS LIB] å¼€å§‹ç”Ÿæˆ: {text[:10]}...")
        async def _gen():
            communicate = edge_tts.Communicate(text, voice, rate=rate, pitch=pitch)
            await communicate.save(output_path)
        
        # åˆ›å»ºç‹¬ç«‹çš„äº‹ä»¶å¾ªç¯ï¼Œé¿å…ä¸ Flask/SocketIO å†²çª
        new_loop = asyncio.new_event_loop()
        asyncio.set_event_loop(new_loop)
        new_loop.run_until_complete(_gen())
        new_loop.close()
        
        logging.info(f"[TTS LIB] ç”ŸæˆæˆåŠŸ")
        return True
    except Exception as e:
        logging.error(f"[TTS LIB] é”™è¯¯: {e}")
        return False

def bg_tts_task(text, voice, rate, pitch, room=None, sid=None):
    """åå° TTS ä»»åŠ¡"""
    # æ¸…ç†æ–‡æœ¬ä¸­çš„è¡¨æƒ…æ ‡ç­¾
    clean_text = re.sub(r'\[(.*?)\]', '', text).strip()
    if not clean_text: return

    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    fname = f"{uuid.uuid4()}.mp3"
    out_path = os.path.join(AUDIO_DIR, fname)
    
    success = False
    
    # ç­–ç•¥ï¼šä¼˜å…ˆå°è¯•å‘½ä»¤è¡Œï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ° Python åº“
    if run_edge_tts_cmd(clean_text, out_path, voice, rate, pitch):
        success = True
    else:
        logging.warning("TTS å‘½ä»¤è¡Œæ¨¡å¼å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ Python åº“æ¨¡å¼...")
        if run_edge_tts_python(clean_text, out_path, voice, rate, pitch):
            success = True

    if success and os.path.exists(out_path) and os.path.getsize(out_path) > 0:
        url = f"/static/audio/{fname}"
        payload = {'audio': url}
        logging.info(f"ğŸ”Š æ¨é€éŸ³é¢‘äº‹ä»¶: {url}")
        
        if room: 
            socketio.emit('audio_response', payload, to=room, namespace='/')
        elif sid: 
            socketio.emit('audio_response', payload, to=sid, namespace='/')
    else:
        logging.error("âŒ æœ€ç»ˆï¼šéŸ³é¢‘ç”Ÿæˆå¤±è´¥")

# ================= Flask è·¯ç”± =================
@app.route('/')
def idx(): 
    return redirect(url_for('pico_v', v=SERVER_VERSION))

@app.route('/pico/<v>')
def pico_v(v):
    r = make_response(render_template('chat.html'))
    # ç¦ç”¨ç¼“å­˜ï¼Œç¡®ä¿å‰ç«¯æ›´æ–°
    r.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
    return r

@app.route('/update_key', methods=['POST'])
def update_key():
    data = request.json
    new_key = data.get('key', '').strip()
    if not new_key.startswith("AIza"): 
        return jsonify({'success': False, 'msg': 'Key æ ¼å¼é”™è¯¯ï¼Œå¿…é¡»ä»¥ AIza å¼€å¤´'})
    
    global client, CONFIG
    CONFIG['GEMINI_API_KEY'] = new_key
    
    try: 
        client = genai.Client(api_key=new_key)
        # å†™å…¥é…ç½®æ–‡ä»¶
        with open(CONFIG_FILE, "w", encoding='utf-8') as f: 
            json.dump(CONFIG, f, indent=2)
        return jsonify({'success': True})
    except Exception as e: 
        return jsonify({'success': False, 'msg': str(e)})

@app.route('/upload_bg', methods=['POST'])
def upload_bg():
    if 'file' not in request.files: return jsonify({'success': False})
    f = request.files['file']
    if f and '.' in f.filename:
        n = secure_filename(f.filename)
        # æ·»åŠ æ—¶é—´æˆ³é˜²æ­¢é‡å
        final_name = f"{int(time.time())}_{n}"
        f.save(os.path.join(BG_DIR, final_name))
        return jsonify({'success': True})
    return jsonify({'success': False})

@app.route('/upload_model', methods=['POST'])
def upload_model():
    if 'file' not in request.files: return jsonify({'success': False})
    f = request.files['file']
    if f.filename.endswith('.zip'):
        try:
            n = secure_filename(f.filename).rsplit('.', 1)[0].lower()
            p = os.path.join(MODELS_DIR, n)
            # æ¸…ç†æ—§ç›®å½•
            shutil.rmtree(p, ignore_errors=True)
            
            # è§£å‹
            with zipfile.ZipFile(f, 'r') as z: 
                z.extractall(p)
            
            # æ™ºèƒ½ä¿®æ­£è·¯å¾„ (å¦‚æœè§£å‹åå¤šäº†ä¸€å±‚æ–‡ä»¶å¤¹)
            for root, dirs, files in os.walk(p):
                if any(f.endswith('.model3.json') for f in files):
                    if root != p: 
                         for item in os.listdir(root): 
                             shutil.move(os.path.join(root, item), p)
                    break
            return jsonify({'success': True})
        except Exception as e: 
            logging.error(f"æ¨¡å‹ä¸Šä¼ å¤±è´¥: {e}")
            return jsonify({'success': False})
    return jsonify({'success': False})

@app.route('/api/danmaku', methods=['POST'])
def api_danmaku():
    """Bç«™ç›´æ’­å¼¹å¹•å¯¹æ¥æ¥å£"""
    data = request.json
    if not data or 'text' not in data: return jsonify({'success': False})
    
    user = data.get('username', 'Bç«™å¼¹å¹•')
    msg = data.get('text', '')
    
    # è®°å½•åˆ°å†å²
    user_msg_obj = {'type': 'chat', 'sender': user, 'text': msg}
    GLOBAL_STATE['chat_history'].append(user_msg_obj)
    save_state()
    
    # å¹¿æ’­åˆ°å‰ç«¯
    socketio.emit('chat_message', {'text': msg, 'sender': user}, to='lobby')
    
    # è§¦å‘ AI å›å¤
    socketio.start_background_task(process_ai_response, user, msg)
    return jsonify({'success': True})

# ================= AI é€»è¾‘ =================
users = {}
chatroom_chat = None

def init_chatroom():
    global chatroom_chat
    if not client: return
    # ä½¿ç”¨å½“å‰é…ç½®çš„äººè®¾
    sys_prompt = CURRENT_MODEL.get('persona', "")
    if not sys_prompt: sys_prompt = DEFAULT_INSTRUCTION
    
    try: 
        chatroom_chat = client.chats.create(
            model="gemini-2.5-flash", 
            config={"system_instruction": sys_prompt}
        )
        logging.info("AI èŠå¤©å®¤åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logging.error(f"AI èŠå¤©å®¤åˆå§‹åŒ–å¤±è´¥: {e}")

def process_ai_response(sender, msg, img_data=None, sid=None):
    try:
        if not chatroom_chat: init_chatroom()
        if not client: 
            if sid: socketio.emit('system_message', {'text': 'è¯·å…ˆè®¾ç½® API Key'}, to=sid)
            return
        
        content = []
        if msg: content.append(f"ã€{sender}ã€‘: {msg}")
        
        # å¤„ç†å›¾ç‰‡
        if img_data:
            try:
                if "," in img_data: _, encoded = img_data.split(",", 1)
                else: encoded = img_data
                content.append(types.Part.from_bytes(data=base64.b64decode(encoded), mime_type="image/jpeg"))
            except Exception as e:
                logging.error(f"å›¾ç‰‡å¤„ç†é”™è¯¯: {e}")
            
        resp = chatroom_chat.send_message(content)
        
        # è§£ææƒ…æ„Ÿæ ‡ç­¾
        emo = 'NORMAL'
        match = re.search(r'\[(HAPPY|ANGRY|SAD|SHOCK|NORMAL)\]', resp.text)
        txt = resp.text
        if match: 
            emo = match.group(1)
            txt = resp.text.replace(match.group(0), '').strip()
            
        ai_msg = {'type': 'response', 'sender': 'Pico', 'text': txt, 'emotion': emo}
        GLOBAL_STATE['chat_history'].append(ai_msg)
        save_state()
        
        socketio.emit('response', {'text': txt, 'sender': 'Pico', 'emotion': emo}, to='lobby')
        
        # è§¦å‘è¯­éŸ³åˆæˆ
        bg_tts_task(txt, CURRENT_MODEL['voice'], CURRENT_MODEL['rate'], CURRENT_MODEL['pitch'], room='lobby')
        
    except Exception as e:
        logging.error(f"AI å›å¤ç”Ÿæˆé”™è¯¯: {e}")
        if sid: socketio.emit('system_message', {'text': f'AI Error: {e}'}, to=sid)

# ================= Socket.IO äº‹ä»¶ =================
@socketio.on('connect')
def on_connect():
    emit('server_ready', {'status': 'ok'})

@socketio.on('login')
def on_login(d):
    u = d.get('username', '').strip() or "User"
    users[request.sid] = {"username": u, "is_admin": False}
    join_room('lobby')
    
    if not chatroom_chat: init_chatroom()
    
    emit('login_success', {
        'username': u, 
        'current_model': CURRENT_MODEL, 
        'current_background': GLOBAL_STATE.get('current_background', '')
    })
    
    emit('history_sync', {'history': GLOBAL_STATE['chat_history']})
    
    # æ¬¢è¿è¯­éŸ³
    socketio.start_background_task(bg_tts_task, f"æ¬¢è¿ {u}", CURRENT_MODEL['voice'], "+0%", "+0%", sid=request.sid)

@socketio.on('message')
def on_msg(d):
    sid = request.sid
    if sid not in users: return
    
    msg = d.get('text', '')
    img = d.get('image', None)
    sender = users[sid]['username']
    
    # ç®¡ç†å‘˜åé—¨æŒ‡ä»¤
    if "/ç®¡ç†å‘˜" in msg and sender.lower() == "yk":
        users[sid]['is_admin'] = True
        emit('admin_unlocked')
        return

    # è®°å½•ç”¨æˆ·æ¶ˆæ¯
    GLOBAL_STATE['chat_history'].append({'type':'chat', 'sender':sender, 'text':msg, 'image': bool(img)})
    save_state()
    
    emit('chat_message', {'text':msg, 'sender':sender, 'image':img}, to='lobby')
    socketio.start_background_task(process_ai_response, sender, msg, img, sid)

def is_admin(sid): return users.get(sid, {}).get('is_admin', False)

# â˜…â˜…â˜… æ‰¾å›ä¸¢å¤±çš„è¯­éŸ³åˆ—è¡¨é€»è¾‘ (å®Œæ•´ç‰ˆ) â˜…â˜…â˜…
@socketio.on('get_studio_data')
def on_get_data():
    # å®Œæ•´çš„ Edge-TTS æ¨èåˆ—è¡¨
    voices = [
        {"id":"zh-CN-XiaoxiaoNeural", "name":"ğŸ‡¨ğŸ‡³ æ™“æ™“ (å¥³å£°)"},
        {"id":"zh-CN-YunxiNeural", "name":"ğŸ‡¨ğŸ‡³ äº‘å¸Œ (å°‘å¹´)"},
        {"id":"zh-CN-YunjianNeural", "name":"ğŸ‡¨ğŸ‡³ äº‘å¥ (æ–°é—»)"},
        {"id":"zh-CN-XiaoyiNeural", "name":"ğŸ‡¨ğŸ‡³ æ™“ä¼Š (å¯çˆ±)"},
        {"id":"zh-TW-HsiaoChenNeural", "name":"ğŸ‡¹ğŸ‡¼ æ™“è‡» (å°æ¹¾)"},
        {"id":"zh-HK-HiuMaanNeural", "name":"ğŸ‡­ğŸ‡° æ™“æ›¼ (ç²¤è¯­)"},
        {"id":"en-US-AnaNeural", "name":"ğŸ‡ºğŸ‡¸ Ana (è‹±æ–‡)"},
        {"id":"en-US-GuyNeural", "name":"ğŸ‡ºğŸ‡¸ Guy (è‹±æ–‡ç”·)"},
        {"id":"ja-JP-NanamiNeural", "name":"ğŸ‡¯ğŸ‡µ ä¸ƒæµ· (æ—¥è¯­)"}
    ]
    emit('studio_data', {
        'models': scan_models(), 
        'current_id': CURRENT_MODEL['id'], 
        'voices': voices, # è¿™é‡ŒæŠŠè¯­éŸ³åˆ—è¡¨è¿”å›ç»™å‰ç«¯
        'backgrounds': scan_backgrounds(), 
        'current_bg': GLOBAL_STATE.get('current_background', '')
    })

@socketio.on('switch_model')
def on_sw(d):
    global CURRENT_MODEL
    t = next((m for m in scan_models() if m['id'] == d['id']), None)
    if t: 
        CURRENT_MODEL = t
        GLOBAL_STATE["current_model_id"] = t['id']
        save_state()
        init_chatroom()
        emit('model_switched', CURRENT_MODEL, to='lobby')

@socketio.on('switch_background')
def on_sw_bg(d):
    GLOBAL_STATE['current_background'] = d.get('name')
    save_state()
    emit('background_update', {'url': f"/static/backgrounds/{d.get('name')}" if d.get('name') else ""}, to='lobby')

@socketio.on('save_settings')
def on_sav(d):
    if not is_admin(request.sid): return
    global CURRENT_MODEL
    try: 
        d['scale']=float(d['scale'])
        d['x']=float(d['x'])
        d['y']=float(d['y'])
    except: pass
    
    updated = save_model_config(d['id'], d)
    if CURRENT_MODEL['id'] == d['id']: 
        CURRENT_MODEL.update(updated)
        init_chatroom()
        emit('model_switched', CURRENT_MODEL, to='lobby')
    emit('toast', {'text': 'âœ… è®¾ç½®å·²ä¿å­˜'})

@socketio.on('delete_model')
def on_del(d):
    if not is_admin(request.sid): return
    if d['id'] != CURRENT_MODEL['id']:
        shutil.rmtree(os.path.join(MODELS_DIR, d['id']), ignore_errors=True)
        emit('toast', {'text': 'ğŸ—‘ï¸ å·²åˆ é™¤'})
        on_get_data()

@socketio.on('download_model')
def on_dl(d):
    if not is_admin(request.sid): return
    name = d.get('name')
    emit('toast', {'text': f'ğŸš€ ä¸‹è½½ {name}...', 'type':'info'})
    socketio.start_background_task(bg_dl_task, name)

def bg_dl_task(name):
    u = f"https://github.com/Live2D/CubismWebSamples/trunk/Samples/Resources/{name}"
    t = os.path.join(MODELS_DIR, name.lower())
    shutil.rmtree(t, ignore_errors=True)
    os.makedirs(t, exist_ok=True)
    try:
        os.system(f"svn export --force -q {u} {t}")
        socketio.emit('toast', {'text': f'âœ… {name} ä¸‹è½½å®Œæˆ!'}, namespace='/')
    except: pass

if __name__ == '__main__':
    logging.info("Starting Pico AI Server...")
    socketio.run(app, host='0.0.0.0', port=5000)
